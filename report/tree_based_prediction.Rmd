---
title: "Tree Based Predictions on Weight Lifting Exercise Dataset"
author: "You-Cyuan Jhang"
date: "June 17, 2014"
output: html_document
---
## Objective

## Setup R Enviroment

First, setup the folder and load the reqired packages. In this report, I will use `data.table` to perform data cleanup. Use `caret` package for maching learning prediction. The `caret` package has a lot of wrapper for maching learning algorithms in R. See [Caret](http://caret.r-forge.r-project.org) and [data.table](http://datatable.r-forge.r-project.org) for more detail.

```{r load_library, results = 'hide', message = FALSE}
library(caret)
library(data.table)
library(randomForest)
library(gbm)
library(glmnet)
project_dir <- "~/Dropbox/Course/2014/Coursera_Practical_Machine_Learning/weight_lifting"
```

## Preprocess Data

### Obtain Data

Download the dataset if file not exist. 

```{r file_download, cache = TRUE}
file_url_train <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
file_url_test <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
file_name_train <- paste0(project_dir, "/data/original/pml-training.csv")
file_name_test <-  paste0(project_dir, "/data/original/pml-testing.csv")
if ( !file.exists(file_name_train) ) {
  dir.create(paste(project_dir, "data/original", sep = "/"))
  download.file(file_url_train, file_name_train)
}
if ( !file.exists(file_name_test) ) {
  download.file(file_url_test, file_name_test)
}
```

Read the weight lifting dataset. The dataset `pml-training.csv` will be use to build machine learning model. I will then use the model to predict testing dataset given in `pml-testing.csv`. 

```{r read_data, cache = TRUE}
nastrings <- c("NA","#DIV/0!","")
wle_data <- fread(paste0(project_dir, "/data/original/pml-training.csv"), na.strings = nastrings)
wle_submission <- fread(paste0(project_dir, "/data/original/pml-testing.csv"), na.strings = nastrings)
```

According to the [paper](http://groupware.les.inf.puc-rio.br/public/papers/2013.Velloso.QAR-WLE.pdf) describing this dataset, they use a sliding window approach to detect and separate each movement measurements. Then the statistcs (mean, variance, skiness..) of this window measure is calculated and stored. Therefore, my first step is to filter out these records from the original dataset. Then remove unrevelent `time stamp`, `record_id`, `new_window`, `num_window` and `user_name`.

```{r collect_window, cache = TRUE}
wle_data <- wle_data[new_window == "yes",]
wle_data <- wle_data[, c("V1", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp", "new_window", "num_window", "user_name") := NULL]
# Convert should be numeric columns to numeric
wle_data <- wle_data[,lapply(.SD, as.numeric), by=c("classe")]
```

### Training-Testing Split

In order to build a machine leraning model and verify my model, I take the dataset and spit into two parts, with 70% of the data for training and 30% of the data for testing. This is processed through `createDataPartition()` function in `caret` package.

```{r train_test_split, cache = TRUE}
set.seed(98736)
train_indices <- createDataPartition(wle_data$classe, p = 0.7)
wle_training <- wle_data[train_indices$Resample1]
wle_testing <- wle_data[-train_indices$Resample1]
```

### Preprocess

Before building machine learning model, the preprocess of the dataset are presented in order as follows:

1. Some of the columns are zero or has near zero values. These features will not be effective when building machine learning model. Here I remove nearzero columns using `nearZeroVar()` function in `caret`.

2. Perform Box-Cox transformation to stabilize variance and make the data more normal distribution-like. Followed by centering and scaling to make the data more list standard normal distribution.

3. Some columns come with value `#DIV/0!` from the original dataset. This is because when computing the statictics in each window, some of the values are missing or zero. I already replace these values with `NA` when reading the original dataset. Here I use the **K Nearest Neighbors** imputation method in `caret` package to fill these missing values. 

4. Finally, Principle Component Analysis are applied. The new fearute variables will have 90% of the variance explained.

```{r preprocess, cache = TRUE}
# 1. Detect nearzero columns and remove them
wle_nearzero <- nearZeroVar(wle_training, saveMetrics = TRUE)
wle_training <- wle_training[,!wle_nearzero$nzv, with = FALSE]
wle_testing <- wle_testing[,!wle_nearzero$nzv, with = FALSE]

# separate y columns
wle_training_y <- as.factor(wle_training$classe)
wle_testing_y <- as.factor(wle_testing$classe)
wle_training <- wle_training[, -1, with = FALSE]
wle_testing <- wle_testing[,-1, with = FALSE]

# 2-4: BoxCox, Center, Scale Impute and PCA
# Impute missing values using k-nearest-neighbors with k = 5
# (Ignore first column since is our outcome variable and it's not numeric)
preprocess_model <- preProcess(wle_training, method = c("BoxCox", "center", "scale", "knnImpute", "pca"), thresh = 0.95, k = 5)
wle_training <- predict(preprocess_model, newdata = wle_training)
wle_testing <- predict(preprocess_model, newdata = wle_testing)
```

After the preprocess step, there are `r preprocess_model$numComp` principle components. These compinents are new features that would be used to build different models.

## Machine Learning Predictions

After the preprocess step, now let's use the training set to build an prediction model. In this section, three different models are compared, which are Random Forest (Bagging) method, Stochastic Gradient Boosting method and logistic regression method (with regularization). Each model are tuned to yield the best prediction accuracy. The best accuracy model are chosen as my final model.

### Random Forest

```{r rf, cache = TRUE}
rf_ctrl <- trainControl(method = "repeatedcv", number = 5, repeats = 5)
rf_grid <- expand.grid(mtry = 3:10)
rf_model <- train(wle_training, wle_training_y, method = "rf", trControl = rf_ctrl, tuneGrid = rf_grid, metric = "Accuracy")
```

```{r rf_summary}
ggplot(rf_model)
rf_testing_y <- predict(rf_model, newdata = wle_testing)
rf_cf <- confusionMatrix(rf_testing_y, wle_testing_y)
```

Contingency table:
```{r rf_cf_table, echo = FALSE}
rf_cf$table
```

Overall Accuracy: 
```{r rf_acc, echo = FALSE} 
rf_cf$overall[1]
```

### Stochastic Gradient Boosting

```{r gbm, cache = TRUE}
gbm_ctrl <- trainControl(method = "repeatedcv", number = 5, repeats = 5)
gbm_grid <- expand.grid(interaction.depth = c(3, 4, 5, 6), n.trees = 1:30*30, shrinkage = 0.2)
gbm_model <- train(wle_training, wle_training_y, method = "gbm", trControl = gbm_ctrl, tuneGrid = gbm_grid, metric = "Accuracy", verbose = FALSE)
```

```{r gbm_summary}
ggplot(gbm_model)
gbm_testing_y <- predict(gbm_model, newdata = wle_testing)
gbm_cf <- confusionMatrix(gbm_testing_y, wle_testing_y)
```

Contingency table:
```{r gbm_cf_table, echo = FALSE}
gbm_cf$table
```

Overall Accuracy: 
```{r gbm_acc, echo = FALSE} 
gbm_cf$overall[1]
```


### Multi-Class Logistic Regression with Regularization

```{r glmnet, cache = TRUE, warning = FALSE}
glmnet_ctrl <- trainControl(method = "repeatedcv", number = 5, repeats = 5)
glmnet_grid <- expand.grid(lambda = 10^(seq(-3,-1,0.1)), alpha = c(0.0, 0.5, 1.0))
glmnet_model <- train(wle_training, wle_training_y, method = "glmnet", trControl = glmnet_ctrl, tuneGrid = glmnet_grid, metric = "Accuracy", family = c("multinomial"))
```

```{r glmnet_summary}
ggplot(glmnet_model) + scale_x_log10()
glmnet_testing_y <- predict(glmnet_model, newdata = wle_testing)
glmnet_cf <- confusionMatrix(glmnet_testing_y, wle_testing_y)
```

Contingency table:
```{r glmnet_cf_table, echo = FALSE}
glmnet_cf$table
```

Overall Accuracy: 
```{r glmnet_acc, echo = FALSE} 
glmnet_cf$overall[1]
```


### Choices and Make Submission

Pick the most accuracy model.

```{r make_submission}
wle_submission <- wle_submission[, c("V1", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp", "new_window", "num_window", "user_name") := NULL]
wle_submission <- wle_submission[,lapply(.SD, as.numeric), by = c("problem_id")]
wle_submission <- wle_submission[,!wle_nearzero$nzv, with = FALSE]
wle_submission_id <- wle_submission$problem_id
wle_submission <- predict(preprocess_model, newdata = wle_submission[, -1, with = FALSE])
wle_submission_y <- predict(rf_model, newdata = wle_submission)
```

```{r save_submission, echo = FALSE}
dir.create("tree_based_prediction_submission")
for (i in wle_submission_id){
  write(as.character(wle_submission_y[i]), file = paste0("tree_based_prediction_submission/", i))
}
```

## Discussion

## Summary